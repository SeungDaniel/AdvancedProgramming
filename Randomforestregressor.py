import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
import seaborn as sns
import matplotlib.pyplot as plt

# === 1ï¸âƒ£ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ===
df = pd.read_csv("green_tea_full_data_final.csv")

# === 2ï¸âƒ£ ë¬¸ìí˜• ë³€ìˆ˜ ì¸ì½”ë”© ===
df["Water_type"] = df["Water_type"].map({"Tap": 0, "Distilled": 1})
df["Tea_form"] = df["Tea_form"].map({"Powder": 1, "Leaves": 0})

# === 3ï¸âƒ£ ë³€ìˆ˜ ì •ì˜ ===
features = ["Water_type", "Tea_form", "Temperature", "Time_min"]
targets = [col for col in df.columns if col not in features]

# === 4ï¸âƒ£ ì €ì¥ êµ¬ì¡° ì¤€ë¹„ ===
results = []
importances = pd.DataFrame(index=features)

# === 5ï¸âƒ£ ëª¨ë¸ í•™ìŠµ ë£¨í”„ ===
for target in targets:
    y = df[target].values
    X = df[features].values

    model = RandomForestRegressor(
        n_estimators=400, random_state=42, n_jobs=-1
    )
    model.fit(X, y)
    y_pred = model.predict(X)

    r2 = r2_score(y, y_pred)
    rmse = np.sqrt(mean_squared_error(y, y_pred))
    results.append({"Target": target, "R2": r2, "RMSE": rmse})

    # Feature importance
    importances[target] = model.feature_importances_

# === 6ï¸âƒ£ ê²°ê³¼ DataFrame ===
results_df = pd.DataFrame(results)
print("\nğŸ“Š ëª¨ë¸ë³„ ì„±ëŠ¥ ìš”ì•½")
print(results_df.sort_values("R2", ascending=False))

# === 7ï¸âƒ£ ë³€ìˆ˜ ì¤‘ìš”ë„ ìˆ˜ì¹˜ ì¶œë ¥ ===
print("\nğŸ”¥ Feature Importance (ìˆ˜ì¹˜ ê¸°ë°˜)")
print(importances.round(3))

# === 8ï¸âƒ£ ê° Targetë³„ Top Feature í•´ì„ ===
print("\nğŸ§  ë³€ìˆ˜ë³„ ì˜í–¥ë ¥ ìš”ì•½")
for t in importances.columns:
    top_feat = importances[t].idxmax()
    top_val = importances[t].max()
    print(f"{t:<15} â†’ ê°€ì¥ ì˜í–¥ í° ë³€ìˆ˜: {top_feat} ({top_val:.2f})")

# === 9ï¸âƒ£ Heatmap ì‹œê°í™” ===
plt.figure(figsize=(10,6))
sns.heatmap(importances, annot=True, cmap="YlOrBr", fmt=".2f")
plt.title("Feature Importance Heatmap (RandomForest)", fontsize=13)
plt.xlabel("Target Variables")
plt.ylabel("Features")
plt.tight_layout()
plt.show()
